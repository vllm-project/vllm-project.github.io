---
layout: post
title: "Performance boost on ROCm Backend with new Attention Design"
author: "AMD and Embedded LLM"
image: /assets/figures/ptpc/PTPC-tumbnail.png
math: true
---

## Introduction

Attention backend is one of the most compilicated module in vLLM, which is deeply bound to the hardware platform. Even in the same hardware platform, there are some different attention backends to support different user scenarios delivering different performance. In this post, we will introduce the attention backends on AMD's ROCm platform. We will elaborate the implementation priciples of the attention backend, along with the corrsponding benchmarking perform, aiming to provide a reference for users to select attention backends.  

Generally, the attention modules of common large language models can be categorized into two main types: Multi-Head Attention (MHA) and Multi-Latent Attention. Qwen/Llama are using MHA module, while Deepseek/Kimi are using MLA module. In vLLM, both the two kinds of attention modules are supported on ROCm backend. Even more, there are four multi-head attention (MHA) backend and two Multi-Latent Attention (MLA) backend supported on AMD's platform, which can be summarized as following and users and enable each of them with the corresponding backend selection.

| Category | Backend | How to enable |
|:----------|:----------------|:--------------|
| MHA | TRITON_ATTN | export VLLM_ATTENTION_BACKEND=TRITON_ATTN |
| MHA | ROCM_AITER_UNIFIED_ATTN | export VLLM_ATTENTION_BACKEND=ROCM_AITER_UNIFIED_ATTN |
| MHA | ROCM_ATTN | export VLLM_ATTENTION_BACKEND=ROCM_ATTN |
| MHA | ROCM_AITER_FA | export VLLM_ATTENTION_BACKND=ROCM_AITER_FA |
| MLA | TRITON_MLA | export VLLM_ATTENTION_BACKEND=TRITON_MLA |
| MLA | ROCM_AITER_MLA | export VLLM_ATTENTION_BACKEND=ROCM_AITER_MLA |

## Multi-Head Attention Backend

### The Unified Attention backend 

The principle of the unified attention backend is to first write the key and value generated by new tokens into the KV cache buffer, then read all the KV cache (including the new token KV buffer and the context KV buffer) to compute the attention output by invoking the attention operator once. Regardless of whether chunk prefill occurs, the attention operator is called only once, hence the name "unified attention." 

vLLM ROCm backend now supports two unified attention implementations: one is Triton unified attention  https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/triton_attn.py#L265, and the other is Aiter unified attention https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/rocm_aiter_unified_attn.py . Both the implementations operate on the same principle but use different kernelsâ€”the former invokes the default vLLM Triton attention kernel https://github.com/vllm-project/vllm/blob/main/vllm/attention/ops/triton_unified_attention.py#L735, while the latter calls the AITER Triton attention kernel.

```python
# Stage 1: Save the Key/Value of new tokens into KV-Cache
reshape_and_cache(new_key, new_value, ...)  kv_len=3616

# Stage 2: Read the KV-Cache (new Key/Value and history Key/Value) for attention computation
unified_attention(new_qeury, KV-Cache, ...)  kv_cache_len=20000
```

### The ROCM AITER FA backend 

This attention backend utilizes the AITER high-performance MHA kernel for computation, hence named as AITER Multi-head Attention backend. This backend can be employed by models with MHA/GQA structured attention modules during the prefill phase, such as Llama and Qwen models. 

**Design Overview** 
![AITER FA Backend](/assets/figures/2025-12-16-rocm-attention-backend/AiterFaBackend.png)

The backend's performance leap stems from two fundamental design choices that re-think standard attention computation:

***Request Routing***: It dynamically categorizes and processes incoming requests into three distinct paths: Prefill, Decode, and Extend. This allows for specialized, optimized kernels to be applied to each specific workload type.

***Hardware-Optimized KV Cache Layout***: The backend abandons the standard KV cache layout in favor of a shuffled format designed for aiter's `pa_asm` (an assembly-optimized paged attention kernel). The new layouts are:

`k_cache: [num_blocks, num_heads, head_dim // x, block_size, x]`

`v_cache: [num_blocks, num_heads, block_size // x, head_dim, x]`
This reorganization ensures the `pa_asm` kernel experiences the most efficient memory access patterns possible, minimizing latency.

**Performance-Optimized Processing Paths**
Requests are reordered and processed in a decode:extend:prefill sequence, with each type taking a distinct, optimized path:

ğŸ” **Decode Path (For Single Token Generation)**: Leverages the new shuffled KV cache layout directly. A custom `reshape_and_cache` kernel ensures the cache is always in the optimal format, allowing the backend to call pa_asm with zero layout conversion overhead. This is the source of the **15-20% decode throughput improvement**.

âœ¨ **Prefill Path (For Processing New Prompts)**: Uses the highly optimized `flash_attn_varlen_func` directly, as the standard `[num_tokens, num_heads, head_dim]` layout is already ideal for this operation. Results are written in-place to avoid any extra memory copies.

ğŸ”„ **Extend Path (For Context Extension)**: Presents a unique challenge. The shuffled layout is poor for long-sequence computation, but we don't want to lose its decode benefits. Our solution is a KV Cache Fetcher:

- New tokens are computed using `flash_attn_varlen_func`.

- The existing context is fetched in chunks, reordered into a prefill-friendly layout, and computed.

- `Attention outputs (attn_out)` and `log-sum-exp values (lse)` from new and cached tokens are merged to produce the final, correct output.
This chunked, buffered approach prevents out-of-memory (OOM) errors with extremely long sequences while maintaining high performance.

**Pseudo-Code for the Extend Path** :
```python
def extend_forward():
    # Stage 1: Attention for new tokens
    flash_attn_varlen_func()

    # Stage 2: Context Chunk Loop Processing
    for chunk in context_chunks: 
        flash_attn_varlen_func() 
        cp_mha_gather_cache()
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

All those designs and implementation can unlock significant AMD GPU performance.

## Multi Latent Attention Backend
### TRITON MLA Backend
The Common.py vllm/vllm/v1/attention/backends/mla/common.py at v0.11.2 Â· vllm-project/vllm æ˜¯æ‰€æœ‰ç¡¬ä»¶backendè®¡ç®—MLA Prefillçš„å…¥å£ã€‚å¯¹äºlong contextçš„è®¡ç®—ï¼Œæ•´ä½“é€»è¾‘ä¸Multi-head attentionæ¥è¿‘ã€‚

query_len=3616, kv_cache_len=20000

```python
def _forward_prefill():
    # Stage 1: Attention for new tokens
    _run_prefill_new_tokens()  3616x3616x3616

    # Stage 2: Context Chunk Loop Processing  
    for chunk in context_chunks:  16384è¢«åˆ‡æˆå¤šä»½å¾ªç¯
        gather_and_maybe_dequant_cache()
        _run_prefill_context_chunk()  # _run_prefill_context_chunk_fa in rocm backend
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

### ROCM AITER MLA Backend
å¦‚æœé€‰æ‹©ä½¿ç”¨aiter mla backendï¼Œaiter mla backendä¼šé‡è½½_flash_attn_varlen_diff_headdimsæ ¸å¿ƒå‡½æ•°ï¼Œä½¿ç”¨AITER MHA kernelè¿›è¡Œattentionè®¡ç®—ã€‚


## Performance Benchmark

### MHA attention performance comparision
### MLA attention performance comparision



## Get Started


