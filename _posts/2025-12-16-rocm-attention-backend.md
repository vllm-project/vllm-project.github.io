---
layout: post
title: "Performance boost on ROCm Backend with new Attention Design"
author: "AMD and Embedded LLM"
image: /assets/figures/ptpc/PTPC-tumbnail.png
math: true
---

## Introduction

Attention backend is one of the most compilicated module in vLLM. Different models have different attention structure. Generally, the attention modules of common large language models can be categorized into two main types: Multi-Head Attention (MHA) and Multi-Latent Attention (MLA). Qwen/Llama are using MHA module, while Deepseek/Kimi are using MLA module. 

Besides the model structure, different hardware platforms implements the attention backends in different way. Even in the same hardware platform, there are some different attention backends to support different user scenarios delivering different performance. In this post, we will introduce the AMD's ROCm attention backend in vLLM. We will elaborate the principles of the attention backend implementation, especially the ROCM_AITER_FA and ROCM_AITER_MLA, along with the corrsponding benchmark performance. This post aims to provide a reference for vLLM users to select the best attention backends on AMD platform.  

In vLLM for AMD ROCm backend, there are four multi-head attention (MHA) implementations and two Multi-Latent Attention (MLA) implementations. For MHA, there are `TRITON_ATTN`, `ROCM_AITER_UNIFIED_ATTN`, `ROCM_ATTN`, and the `ROCM_AITER_FA`. For MLA, there are `TRITON_MLA` and `ROCM_AITER_MLA` attentions. vLLM suggests using the `--attn-backend` to select the corresponding attention backend which can be summarized as in the following table for the attention selection on AMD's platform.

| Category | Backend | How to enable |
|:----------|:----------------|:--------------|
| MHA | TRITON_ATTN | --attn-backend TRITON_ATTN |
| MHA | ROCM_AITER_UNIFIED_ATTN | --attn-backend ROCM_AITER_UNIFIED_ATTN |
| MHA | ROCM_ATTN | --attn-backend ROCM_ATTN |
| MHA | ROCM_AITER_FA | --attn-backend ROCM_AITER_FA |
| MLA | TRITON_MLA | --attn-backend TRITON_MLA |
| MLA | ROCM_AITER_TRITON_MLA | --attn-backend ROCM_AITER_TRITON_MLA |
| MLA | ROCM_AITER_MLA | --attn-backend ROCM_AITER_MLA |

## Multi-Head Attention Backend


### The Unified Attention backend 

As descrbed above, there are four kinds of attention implementations for MHA. The first three `TRITON_ATTN`, `ROCM_AITER_UNIFIED_ATTN`, `ROCM_ATTN` are the unified attention, which compute the prefill and decode in the same sheduleld output in a single kernel.

The principle of the unified attention backend is to first write the key and value of the scheduled tokens generated by QKV gemm into the KV cache buffer, then read all of the KV cache (including the scheduled token KV cache and the context KV cache) to compute the attention output by invoking the attention kernel only once, regardless of whether chunked prefill occurs.

vLLM ROCm backend now supports three unified attention implementations: one is Triton unified attention  https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/triton_attn.py#L265, which is developed by vllm community, and the other is Aiter unified attention https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/rocm_aiter_unified_attn.py . Both the implementations follows the same principle but use the different kernels‚Äîthe former invokes the default vLLM Triton attention kernel https://github.com/vllm-project/vllm/blob/main/vllm/attention/ops/triton_unified_attention.py#L735, while the latter calls the AITER Triton attention kernel.

Figure 1(a) illustrate the detailed principle of the unified attention. The server receives many requests from the client and the schduler contiguously batch several requests together including the decode/prefill/extended-prefill toghether, among which the extended-prefill happens in chunked prefill scenario. They are batched together and Query/Key/Value features are computed at first then being used in Attention backend. For unified attention, the Query/Key/Value features from all the prefill/extend/decode are combined together for compution.

Taking one request of the ISL=80000, OSL=1, `num_batch_tokens=32768`, `max_concurrency=1` for example. It will takes three forward steps to finish the prefill computation for all the 80000 tokens.


Below is the pseudo-code illustrating how unified attention works under the chunked prefill scenario:
```python
# Step 1: Schedule a single request, whose prompt length is 80000
# vllm server is launched by enabling chunked prefill and max num batched tokens is specified as 32768
# so the first step is only 32768 tokens are scheduled for this request

# Stage 1: Save the Key/Value of new tokens of this new request into KV-Cache
reshape_and_cache(key, value, ...)  # key/value in shape of [32768, hidden_size]

# Stage 2: Read the KV-Cache for attention computation
unified_attention(query, KV-Cache, ...) # query in shape of [32768, hidden_size]
                                        # KV-Cache in shape of [32768, hidden_size]

# Step 2: 
# In second round step, the seccessive 32768 tokens are scheduled for this request
# Stage 1: Save the Key/Value of new tokens of this new request into KV-Cache
reshape_and_cache(key, value, ...)  # key/value in shape of [32768, hidden_size]

# Stage 2: Read the KV-Cache for attention computation
unified_attention(query, KV-Cache, ...) # query in shape of [32768, hidden_size]
                                        # KV-Cache in shape of [65536, hidden_size]

# Step 3: Schedule the remaining tokens of this request, the remaining tokens is 14464
# Stage 1: Save the Key/Value of remaining tokens into KV-Cache
reshape_and_cache(key, value, ...)  # key/value in shape of [14464, hidden_size]  

# Stage 2: Read the KV-Cache (new Key/Value and history Key/Value) for attention computation
unified_attention(new_query, KV-Cache, ...)  # query in shape of [14464, hidden_size]
                                             # KV-Cache in shape of [80000, hidden_size]
```

### The ROCM AITER FA backend 

This attention backend utilizes the AITER high-performance MHA kernel for computation, hence named as AITER Multi-head Attention backend. This backend can be employed by models with MHA/GQA structured attention modules during the prefill phase, such as Llama and Qwen serial models.

**Design Overview** 
![AITER FA Backend](/assets/figures/2025-12-16-rocm-attention-backend/vLLM-Attention.png)

The backend's performance leap stems from two fundamental design choices that re-think standard attention computation:

***Request Routing***: It dynamically categorizes and processes incoming requests into three distinct paths: Decode, Extend and Prefill. This allows for specialized, optimized kernels to be applied to each specific workload type.

***Hardware-Optimized KV Cache Layout***: The backend abandons the standard KV cache layout in favor of a shuffled format designed for the performant kernel in AITER. The new layouts are:

`k_cache: [num_blocks, num_heads, head_dim // x, block_size, x]`

`v_cache: [num_blocks, num_heads, block_size // x, head_dim, x]`
This reorganization ensures the `pa_asm` kernel experiences the most efficient memory access patterns possible, minimizing latency.

**Performance-Optimized Processing Paths**
Requests are reordered and processed in a decode:extend:prefill sequence, with each type taking a distinct, highly performant kernels:

üîÅ **Decode Path (For Single Token Generation)**: Leverages the new shuffled KV cache layout directly. A custom `reshape_and_cache` kernel ensures the cache is always in the optimal format, allowing the backend to call pa_asm with zero layout conversion overhead. This is the source of the **15-20% decode throughput improvement**.

‚ú® **Prefill Path (For Processing New Prompts)**: Uses the highly optimized `flash_attn_varlen_func` directly, as the standard `[num_tokens, num_heads, head_dim]` layout is already ideal for this operation. Results are written in-place to avoid any extra memory copies.

üîÑ **Extend Path (For Context Extension)**: Presents a unique challenge. The shuffled layout is poor for long-sequence computation, but we don't want to lose its decode benefits. Our solution is a KV Cache Fetcher:

- New tokens are computed using `flash_attn_varlen_func`.

- The existing context is fetched in chunks, reordered into a prefill-friendly layout, and computed.

- `Attention outputs (attn_out)` and `log-sum-exp values (lse)` from new and cached tokens are merged to produce the final, correct output.
This chunked, buffered approach prevents out-of-memory (OOM) errors with extremely long sequences while maintaining high performance.

**Pseudo-Code for the Extend Path** :
```python
def extend_forward():
    # Stage 1: Attention for new tokens
    flash_attn_varlen_func()

    # Stage 2: Context Chunk Loop Processing
    for chunk in context_chunks: 
        cp_mha_gather_cache()
        flash_attn_varlen_func()
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

All those designs and implementation can unlock significant AMD GPU performance.

## Multi Latent Attention Backend
For model with MLA structure will choose this attention backend. In vLLM, the general MLA backend is the `TRITON_MLA` backend and will be enabled by default for ROCm platform. Besides, we have defined a new backend as `ROCM_AITER_MLA` backend to leverage the performant kernel in AITER.

### TRITON MLA Backend
The implementation principle of the `TRITON_MLA` backend are similar to the `ROCM_ATIER_FA` backend. It seperates the decode path from the prefill/extend path and use different implementation recipe. For prefill/extend path, the non-absorbed recipe is adopt and it use the standard MHA for computation. However, for decode, it use the absorbed recipe and use the MLA kernel for computation. The `TRITON_MLA` backend use the vLLM default Triton kernel for MHA computation in prefill phase, while it doesn't provide the decode implementation and leave it to the `ROCM_AITER_MLA` backend.

**Pseudo-Code for the Prefill/Extend Path** :
```python
def _forward_prefill():
    # Stage 1: Attention for new tokens
    _run_prefill_new_tokens()  

    # Stage 2: for extend path, Context Chunk Loop Processing  
    for chunk in context_chunks:
        gather_and_maybe_dequant_cache()
        _run_prefill_context_chunk()  # _run_prefill_context_chunk_fa in rocm backend
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

### ROCM AITER MLA Backend

`AiterMLABackend` is a performance-optimized attention backend designed for models like DeepSeek that utilize MLA for their attention computation. This backend delivers significant inference speedups on AMD hardware through deeply optimized assembly kernels, `flash_attn_varlen_func` for the prefill phase, `mla_decode_fwd` for the decode phase

Beyond raw kernel performance, this backend inherits the full feature set of the FlashMLABackend, including PIECEWISE_AND_FULL CUDA graph support and MTP support.one other advantage is its near-identical performance across virtually any KV cache block size. In practice, this means you can treat every token as prefix cache without worrying about the performance penalties typically associated with fine-grained caching‚Äîenabling highly efficient long-context and multi-turn deployments.


## Performance Benchmark

### MHA attention performance comparision
### MLA attention performance comparision



## Get Started


