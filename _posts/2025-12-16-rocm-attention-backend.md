---
layout: post
title: "Performance boost on ROCm Backend with new Attention Design"
author: "AMD and Embedded LLM"
image: /assets/figures/ptpc/PTPC-tumbnail.png
math: true
---

## Introduction

Attention backend is one of the most compilicated module in vLLM, which is deeply bound to the hardware platform. Even in the same hardware platform, there are some different attention backends to support different user scenarios delivering different performance. In this post, we will introduce the attention backends on AMD's ROCm platform. We will elaborate the implementation priciples of the attention backend, along with the corrsponding benchmarking perform, aiming to provide a reference for users to select attention backends.  

Generally, the attention modules of common large language models can be categorized into two main types: Multi-Head Attention (MHA) and Multi-Latent Attention. Qwen/Llama are using MHA module, while Deepseek/Kimi are using MLA module. In vLLM, both the two kinds of attention modules are supported on ROCm backend. Even more, there are four multi-head attention (MHA) backend and two Multi-Latent Attention (MLA) backend supported on AMD's platform, which can be summarized as following and users and enable each of them with the corresponding backend selection.

| Category | Backend | How to enable |
|:----------|:----------------|:--------------|
| MHA | TRITON_ATTN | export VLLM_ATTENTION_BACKEND=TRITON_ATTN |
| MHA | ROCM_AITER_UNIFIED_ATTN | export VLLM_ATTENTION_BACKEND=ROCM_AITER_UNIFIED_ATTN |
| MHA | ROCM_ATTN | export VLLM_ATTENTION_BACKEND=ROCM_ATTN |
| MHA | ROCM_AITER_FA | export VLLM_ATTENTION_BACKND=ROCM_AITER_FA |
| MLA | TRITON_MLA | export VLLM_ATTENTION_BACKEND=TRITON_MLA |
| MLA | ROCM_AITER_MLA | export VLLM_ATTENTION_BACKEND=ROCM_AITER_MLA |

## Multi-Head Attention Backend

### The Unified Attention backend 

The principle of the unified attention backend is to first write the key and value generated by new tokens into the KV cache buffer, then read all the KV cache (including the new token KV buffer and the context KV buffer) to compute the attention output by invoking the attention operator once. Regardless of whether chunk prefill occurs, the attention operator is called only once, hence the name "unified attention." 

vLLM ROCm backend now supports two unified attention implementations: one is Triton unified attention  https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/triton_attn.py#L265, and the other is Aiter unified attention https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/rocm_aiter_unified_attn.py . Both the implementations operate on the same principle but use different kernels—the former invokes the default vLLM Triton attention kernel https://github.com/vllm-project/vllm/blob/main/vllm/attention/ops/triton_unified_attention.py#L735, while the latter calls the AITER Triton attention kernel.

```python
# Stage 1: Save the Key/Value of new tokens into KV-Cache
reshape_and_cache(new_key, new_value, ...)  kv_len=3616

# Stage 2: Read the KV-Cache (new Key/Value and history Key/Value) for attention computation
unified_attention(new_qeury, KV-Cache, ...)  kv_cache_len=20000
```

### The ROCM AITER FA backend 

This attention backend utilizes the AITER high-performance MHA kernel for computation, hence named as AITER Multi-head Attention backend. This backend can be employed by models with MHA/GQA structured attention modules during the prefill phase, such as Llama and Qwen models. It invokes the AITER flash_attn_varlen_func function and launches ASM kernel for attention computation. For long-context with prefix-caching or chunked-prefill scenarios, it will goes into the extend_forward path vllm/vllm/v1/attention/backends/rocm_aiter_fa.py at v0.11.2 · vllm-project/vllm. The corresponding pseudocode of the workflow can be represented by

```python
def extend_forward():
    # Stage 1: Attention for new tokens
    flash_attn_varlen_func()  3616x3616x3616

    # Stage 2: Context Chunk Loop Processing
    for chunk in context_chunks:  16384被切成多份循环
        flash_attn_varlen_func() 
        cp_mha_gather_cache()
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

## Multi Latent Attention Backend
### TRITON MLA Backend
The Common.py vllm/vllm/v1/attention/backends/mla/common.py at v0.11.2 · vllm-project/vllm 是所有硬件backend计算MLA Prefill的入口。对于long context的计算，整体逻辑与Multi-head attention接近。

query_len=3616, kv_cache_len=20000

```python
def _forward_prefill():
    # Stage 1: Attention for new tokens
    _run_prefill_new_tokens()  3616x3616x3616

    # Stage 2: Context Chunk Loop Processing  
    for chunk in context_chunks:  16384被切成多份循环
        gather_and_maybe_dequant_cache()
        _run_prefill_context_chunk()  # _run_prefill_context_chunk_fa in rocm backend
        merge_attn_states()

    # Stage 3: Get the final result
    merge_attn_states()
```

### ROCM AITER MLA Backend
如果选择使用aiter mla backend，aiter mla backend会重载_flash_attn_varlen_diff_headdims核心函数，使用AITER MHA kernel进行attention计算。


## Performance Benchmark

### MHA attention performance comparision
### MLA attention performance comparision



## Get Started


